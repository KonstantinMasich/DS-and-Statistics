{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строгая экзогенность\n",
    "<br>\n",
    "- **Вопрос 1:** каково влияние стохастического шума на оценки коэффициентов регрессии?\n",
    "- **Вопрос 2:** если в целевой функции две зависимых переменных, а в модели построена только одна, то насколько конкретно будет смещена оценка коэффициента перед переменной в модели? Рассмотрим два случая: когда имеется confounding-переменная (т.е. истинная причина, которая влияет на зависимую и независимую переменные), и когда имеется просто недоступная нам переменная, которая не влияет на имеющуюся у нас переменную, однако при этом её матожидание не равно нулю, а оттуда и матожидание шума получится ненулевое, то есть предположение о строгой экзогенности будет нарушено.\n",
    "- **Вопрос 3:** если в целевой функции $\\text{y}$ зависит от $\\text{X и X^2}$, а в модели - только от $\\text{X}$ (то есть $\\text{X^2}$ окажется в шуме), то насколько конкретно будет смещена оценка коэффициента перед переменной в модели?\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><hr>\n",
    "## 1. Наблюдение первое - влияние шума на оценки\n",
    "\n",
    "Сначала рассмотрим влияние стохастического шума на оценки коэффициентов регрессии $\\hat{\\beta}$ на примере наклона -  $\\hat{\\beta_1}$.\n",
    "\n",
    "По Предположению 1 (линейность), цель $y$ имеет вид\n",
    "\n",
    "$$y_i = \\underbrace{f(x_i)}_{\\text{сигнал}} + \\underbrace{\\varepsilon_i}_{\\text{шум}}$$\n",
    "\n",
    "Чисто для удобства нотации запишем цель как $y_i+\\varepsilon_i$ (не будет никакой проблемы продолжить писать $f(x_i)$, это просто немного длиннее).\n",
    "\n",
    "Выведем оценку наклона $\\hat{\\beta_1}$, учитывая стохастический шум:\n",
    "\n",
    "$$RSS=\\sum(y_i+\\varepsilon_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)^2=\\sum{( y_i^2+\\varepsilon_iy_i-\\hat{\\beta_0}y_i-\\hat{\\beta_1}x_iy_i+\\varepsilon_iy_i+\\varepsilon_i^2-\\hat{\\beta_0}\\varepsilon_i-\\hat{\\beta_1}\\varepsilon_ix_i-\\hat{\\beta_0}y_i-\\hat{\\beta_0}\\varepsilon_i+\\hat{\\beta_0}^2+\\hat{\\beta_0}\\hat{\\beta_1}x_i-\\hat{\\beta_1}x_iy_i-\\hat{\\beta_1}\\varepsilon_ix_i+\\hat{\\beta_0}\\hat{\\beta_1}x_i+\\hat{\\beta_1}^2x_i^2)}=\\sum{( y_i^2 + \\varepsilon_i^2 + \\hat{\\beta_0}^2 + \\hat{\\beta_1}^2x_i^2+2\\varepsilon_iy_i-2\\hat{\\beta_0}y_i-2\\hat{\\beta_1}x_iy_i-2\\hat{\\beta_0}\\varepsilon_i-2\\hat{\\beta_1}\\varepsilon_ix_i+2\\hat{\\beta_0}\\hat{\\beta_1}x_i )}$$\n",
    "\n",
    ">Примечание: суммирование здесь и далее происходит согласно $\\sum_{i=1}^n$, где $n$ - объём обучающей выборки.\n",
    "\n",
    "Взяв частные производные по весам, получим:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_0}}RSS=2\\sum{\\hat{\\beta_0}}-2\\sum{y_i}-2\\sum{\\varepsilon_i}+2\\hat{\\beta_1}\\sum{x_i}$$\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_1}}RSS=2\\hat{\\beta_1}\\sum{x^2_i}-2\\sum{x_iy_i}-2\\sum{x_i\\varepsilon_i}+2\\hat{\\beta_0}\\sum{x_i}$$\n",
    "\n",
    "Для упрощения нотации запишем:\n",
    "\n",
    "$$S_X=\\sum{x_i}, S_Y=\\sum{y_i}, S_{YX}=\\sum{y_ix_i}, S_{XX}=\\sum{x^2_i}, nb_0=\\sum{\\hat{\\beta_0}}, S_E=\\sum{\\varepsilon_i}, S_{XE}=\\sum{x_i\\varepsilon_i}$$\n",
    "\n",
    "Приравняв производные к нулю, получим:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_0}}RSS=0=nb_0-S_Y-S_E+\\hat{\\beta_1}S_X \\implies b_0=\\frac{S_Y-\\hat{\\beta_1}S_X+S_E}{n}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_1}}RSS=0=\\hat{\\beta_1}S_{XX}-S_{YX}-S_{XE}+b_0S_X$$\n",
    "\n",
    "Подставив выражение для $\\hat{\\beta_0}$ в выражение для  $\\hat{\\beta_1}$, получим:\n",
    "\n",
    "$$0 = \\hat{\\beta_1}S_{XX}-S_{YX}+S_X\\frac{S_Y-\\hat{\\beta_1}S_X+S_E}{n}-S_{XE}$$\n",
    "$$0 = n\\hat{\\beta_1}S_{XX}-nS_{YX}+S_XS_Y-\\hat{\\beta_1}S_XS_X+S_XS_E-nS_{XE}$$\n",
    "<br>\n",
    "$$\\hat{\\beta_1} = \\frac{S_XS_Y-nS_{XY} +S_XS_E-nS_{XE}}{S_XS_X-nS_{XX}}=\\frac{\\text{Cov}(X, Y)+\\text{Cov}(X, E)}{s^2_X}$$\n",
    "<br>\n",
    "\n",
    "В последней формуле $s^2_X$ означает sample variance $[X]$.\n",
    "\n",
    "Легко заметить, что полученную формулу можно переписать как\n",
    "<br>\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\text{Cov}(X, Y)}{s^2_X} + \\frac{\\text{Cov}(X, E)}{s^2_X} = \\beta_1 + \\frac{\\text{Cov}(X, E)}{s^2_X}$$\n",
    "\n",
    "Получилось так называемое \"Constant + Noise Representation\": оценка $\\hat{\\beta_1}$ может быть записана как сложение константы $\\beta_1$ и шума, который мы записали в виде суммы некоррелированных случайных величин.\n",
    "\n",
    "Другой способ прийти к такому представлению оценки $\\hat{\\beta_1}$ тоже прост. Ранее мы уже вывели формулу: в отстутствие шума, $\\hat{\\beta_1} = \\frac{\\text{Cov}(X, Y)}{s^2_X}$. Распишем её обратно в более подробный вид:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\text{Cov}(X, Y)}{s^2_X} = \\frac{\\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{s_X^2} = \\frac{\\frac{1}{n}\\sum(x_i-\\bar{x})y_i - \\frac{1}{n}\\sum(x_i-\\bar{x})\\bar{y}}{s_X^2}$$\n",
    "\n",
    "Обратим внимание, что для любой переменной z верно следующее: средняя разность z от её выборочного среднего равна нулю. В самом деле:\n",
    "\n",
    "$$\\frac{1}{n}\\sum(z_i-\\bar{z})=\\frac{1}{n}\\sum{z_i}-\\frac{1}{n}\\sum\\bar{y}=\\bar{z}-\\frac{n\\bar{z}}{n}=\\bar{z}-\\bar{z}=0$$\n",
    "\n",
    "Отсюда:\n",
    "\n",
    "$$\\hat{\\beta_1}=\\frac{\\frac{1}{n}\\sum(x_i-\\bar{x})y_i - \\bar{y}\\cdot\\overbrace{\\frac{1}{n}\\sum(x_i-\\bar{x})}^{=0}}{S_X^2} = \\frac{\\frac{1}{n}\\sum(x_i-\\bar{x})y_i}{s_X^2}$$\n",
    "\n",
    "Здесь в числителе $y_i$ совершенно не обязательно будет константой (даже наоборот - почти наверняка не будет), поэтому мы не можем воспользоваться приведённой выше формулой. Теперь воспользуемся Предположением 1, и распишем $y_i$  как $y_i=\\beta_0+\\beta_1 x_i + \\varepsilon_i = \\beta_0+\\beta_1\\bar{x}+\\beta_1(x_i-\\bar{x}) + \\varepsilon_i$. Разумеется, такая запись легальна, потому что мы просто прибавляем и вычитаем $\\beta_1\\bar{x}$, не более того. Подставим:\n",
    "\n",
    "$$\\hat{\\beta_1}=\\frac{\\frac{1}{n}\\sum(x_i-\\bar{x})(\\beta_0+\\beta_1\\bar{x}+\\beta_1(x_i-\\bar{x}) + \\varepsilon_i)}{S_X^2}$$\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{ \\frac{\\beta_0+\\beta_1\\bar{x}}{n}\\sum(x_i-\\bar{x}) + \\beta_1\\frac{1}{n}\\sum(x_i-\\bar{x})^2 + \\frac{1}{n}(x_i-\\bar{x})\\varepsilon_i }{s^2_X}$$\n",
    "\n",
    "Первая сумма в числителе равну нулю (по формуле выше), вторая сокращается до $\\beta_1$. В итоге остаёмся с \n",
    "\n",
    "$$\\hat{\\beta_1} = \\beta_1 + \\frac{\\sum(x_i-\\bar{x})\\varepsilon_i}{ns^2_X}$$\n",
    "\n",
    "что легко можно переписать как \n",
    "\n",
    "$$\\hat{\\beta_1} = \\beta_1 + \\frac{\\sum(x_i-\\bar{x})\\varepsilon_i}{ns^2_X} = \\beta_1 + \\frac{\\sum(x_i-\\bar{x})(\\varepsilon_i-\\bar{\\varepsilon})}{ns^2_X} = \\beta_1 + \\frac{\\text{Cov}(X, E)}{s^2_X}$$\n",
    "\n",
    "Рассмотрим влияние стохастического шума. Допустим, мы подгоняем модель ко всей генеральной совокупности (чтобы исключить влияние sample variance и рассмотреть только влияние стохастического шума). В данном случае мы воспринимаем шум чисто как погрешность измерений - например, некоторая погрешность в показаниях датчика скорости. Насколько оценка $\\hat{\\beta_1}$, вычисленная на чистых данных, будет отличаться от оценки $\\hat{\\beta_1}$, вычисленной на зашумлённых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      " Сгенерирована целевая функция. Шум отсутствует.\n",
      "    Значения коэффициентов целевой функции:\n",
      "\t· β0 = 4.5 \n",
      "\t· β1 = 80.32439239333253\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Добавлен гауссовский шум.\n",
      "    Матожидание шума E[epsilon]:\n",
      "\t\tРеальное     : 8 \n",
      "\t\tРассчитанное : 8.079353970574596 (на 100 наблюдений)\n",
      "    Энергия шума   Var[epsilon]:\n",
      "\t\tРеальная     : 100 \n",
      "\t\tРассчитанная : 86.37830500032116 (на 100 наблюдений)\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Произведена подгонка модели.\n",
      "\t· Оценка β0: 12.464157382966832\n",
      "\t· Оценка β1: 81.13754668594436\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Выполнены рассчёты.\n",
      "\tОценка β1 на чистых данных : 80.32439239333253\n",
      "\tОценка β1 на шумных данных : 81.13754668594436\n",
      "\tРеальная разность β        : 0.8131542926118271\n",
      "\tВычисленная разность β     : 0.8131542926118247\n",
      "\n",
      "\tТаким образом, согласно разложению:\n",
      "\n",
      "\t\t\t                   Cov(X, E)   \n",
      "\t\t\tОценка_β1 = β1 + ——————————————\n",
      "\t\t\t                   (std(X))^2  \n",
      "\n",
      "\tИмеем:\n",
      "\t\t\tОценка_β1 = 81.13754668594436\n",
      "\n",
      "\n",
      "Рассчитанное матожидание резидуалов: -1.3624656958199921e-14\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import numpy as np\n",
    "# Graphics:\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "# ML:\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import make_regression\n",
    "# Wargnings:\n",
    "np.warnings.filterwarnings(\"ignore\")\n",
    "from pandas.core import datetools\n",
    "np.warnings.resetwarnings()\n",
    "\n",
    "PRINT_LINE = '-'*80 + '\\n'\n",
    "\n",
    "POPULATION_SIZE = 100\n",
    "BIAS       = 4.5\n",
    "NOISE_TYPE = 'gaussian' # or 'uniform'\n",
    "LOW, HI    = -10, 55\n",
    "NOISE_MEAN, NOISE_STD = 8, 10\n",
    "\n",
    "# 1. Сгенерируем данные без шума:\n",
    "X, y, real_coef = make_regression(n_samples=POPULATION_SIZE, n_features=1, n_informative=1, \n",
    "                                  coef=True, noise=0, bias=BIAS)\n",
    "print(PRINT_LINE, 'Сгенерирована целевая функция. Шум отсутствует.\\n    Значения коэффициентов целевой функции:')\n",
    "print('\\t· β0 =', BIAS, '\\n\\t· β1 =', real_coef.item(0))\n",
    "\n",
    "# 2. Добавим шум вручную (это позволит нам сохранить его для исследования)\n",
    "if NOISE_TYPE == 'gaussian':\n",
    "    epsilon, typestring = np.random.normal(NOISE_MEAN, NOISE_STD, y.shape), 'гауссовский'\n",
    "    mu, std = NOISE_MEAN, NOISE_STD\n",
    "elif NOISE_TYPE == 'uniform':\n",
    "    epsilon, typestring = np.random.uniform(LOW, HI, y.shape), 'равномерный'\n",
    "    mu, std = 0.5(LOW + HI), np.sqrt(((HI-LOW)**2)/12)\n",
    "\n",
    "y_noisy = y + epsilon\n",
    "print('\\n', PRINT_LINE, 'Добавлен', typestring, 'шум.')\n",
    "print('    Матожидание шума E[epsilon]:')\n",
    "print('\\t\\tРеальное     :', mu, '\\n\\t\\tРассчитанное :', np.mean(epsilon), \n",
    "      '(на', len(epsilon),'наблюдений)')\n",
    "print('    Энергия шума   Var[epsilon]:')\n",
    "print('\\t\\tРеальная     :', std**2, '\\n\\t\\tРассчитанная :', np.var(epsilon, ddof=1), \n",
    "      '(на', len(epsilon),'наблюдений)',)\n",
    "\n",
    "# 3. Произведём подгонку модели к зашумлённой версии y:\n",
    "statsmodels_result = sm.OLS(y_noisy, sm.add_constant(X)).fit()\n",
    "print('\\n', PRINT_LINE, 'Произведена подгонка модели.\\n\\t· Оценка β0:', statsmodels_result.params[0])\n",
    "print('\\t· Оценка β1:', statsmodels_result.params[1])\n",
    "\n",
    "# 4. Выполним рассчёты:\n",
    "print('\\n', PRINT_LINE, 'Выполнены рассчёты.')\n",
    "print('\\tОценка β1 на чистых данных :', real_coef.item(0))\n",
    "print('\\tОценка β1 на шумных данных :', statsmodels_result.params[1])\n",
    "print('\\tРеальная разность β        :', statsmodels_result.params[1]-real_coef.item(0))\n",
    "cov = np.cov(X[:, 0], epsilon)[0, 1]\n",
    "var = np.var(X, ddof=1)\n",
    "print('\\tВычисленная разность β     :', cov/var)\n",
    "print('\\n\\tТаким образом, согласно разложению:\\n')\n",
    "print('\\t\\t\\t                   Cov(X, E)   ')\n",
    "print('\\t\\t\\tОценка_β1 = β1 + ——————————————')\n",
    "print('\\t\\t\\t                   (std(X))^2  ')\n",
    "print('\\n\\tИмеем:')\n",
    "print('\\t\\t\\tОценка_β1 =', real_coef.item(0) + cov/var)\n",
    "print('\\n\\nРассчитанное матожидание резидуалов:', np.mean(statsmodels_result.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "informative_beta     = max(real_coef.item(0), real_coef.item(1))\n",
    "non_informative_beta = min(real_coef.item(0), real_coef.item(1))\n",
    "\n",
    "x = X[:, informative_feature_index]\n",
    "z = X[:, 1-informative_feature_index]\n",
    "\n",
    "# Длинная запись:\n",
    "# xz_dot = np.dot(x, z)                  # 1st term\n",
    "# z_mean_x = z*np.mean(x)\n",
    "# z_mean_x = sum(z_mean_x)               # 2nd term\n",
    "# norm_x = (np.linalg.norm(x))**2        # 3rd term\n",
    "# n_mean_x_sq = len(x) * (np.mean(x))**2 # 4th term\n",
    "# b1_biased_estimate = non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq))\n",
    "# print(':::', non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq)))\n",
    "\n",
    "# Короткая запись:\n",
    "numerator   = np.dot(np.array(x)-np.mean(x), z)\n",
    "denominator = np.square(np.linalg.norm(x)) - len(x)*np.square(np.mean(x)) \n",
    "b1_biased_estimate = non_informative_beta + informative_beta*(numerator/denominator)\n",
    "\n",
    "print('Biased b1 estimate:', b1_biased_estimate)\n",
    "print('Absolute difference:', np.abs(b1_estimate - b1_biased_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([1, 2]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimated coefficient b would pick up the effect of x on y plus the association of x and z times the effect of z on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z_X = np.delete(X, informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('Effect of NON-INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "# z_X = np.delete(X, 1-informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('\\nEffect of INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "\n",
    "# z_X_1, z_X_2 = X[informative_feature_index], X[1 - informative_feature_index]\n",
    "# statsmodels_result = sm.OLS(z_X_2, sm.add_constant(z_X_1)).fit()\n",
    "# print('\\nEffect of NON-INFORMATIVE on INFORMATIVE:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(z_X_2, z_X_1).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noninformative_coef + (inter_coef * informative_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-1.76683487328 + (-0.073345030119 * 20.9079770399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a)\n",
    "a[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат:**\n",
    "* MSE на тренировочной выборке ожидаемо получилась практически нулевой, в силу того, что подгонка полинома второй степени к двум точкам всегда будет идеальной.\n",
    "* Однако на популяции MSE ожидаемо оказалась выше, с мощной дисперсией. Это обусловлено тем, что через две точки можно провести бесконечное количество парабол, и какая из них будет ближе к целевой функции - абсолютно неизвестно.\n",
    "\n",
    "Теперь проведём такой же опыт, но уберём вариативность, задав как минимум 3 точки для тренировочного набора. Для точного задания параболы необходимо иметь как минимум 3 точки, через которые она проходит; а в силу того, что данные не имеют шума, эти 3 точки всегда будут лежать ровно на целевой параболе. Таким образом, ожидается, что подгонка всегда будет идеальной.\n",
    "\n",
    "<br><br><center>**Опыт 2. Нулевой шум, нулевая вариативность**</center>\n",
    "\n",
    "**Условия:**\n",
    "1. Стохастический шум равен нулю: $\\forall{x}, \\epsilon(x)=0$\n",
    "2. Детерминистский шум равен нулю: $\\mathrm{Complexity}(g)=\\mathrm{Complexity}(f)$\n",
    "3. Вариативность модели нулевая; в данном случае (т.е. в случае с целевой функцией-параболой) объём выборки должен быть $\\geq 3$.\n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- Oжидается нулевая MSE как на тренировке, так и на популяции. Это будет достигнуто за счёт нулевой вариативности, нулевого детерминистского и нулевого стохастического шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
