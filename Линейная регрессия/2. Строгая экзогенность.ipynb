{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0=\\hat{\\beta_1}S_{XX}+S_X\\frac{S_Y-\\hat{\\beta_1}S_X-S_E}{n}-S_{YX}+S_{XE}$$\n",
    "$$0=\\hat{\\beta_1}S_{XX}+\\frac{S_XS_Y}{n}-\\frac{\\hat{\\beta_1}S_XS_X}{n}-\\frac{S_XS_E}{n}-S_{YX}+S_{XE}$$\n",
    "$$0=n\\hat{\\beta_1}S_{XX}+S_XS_Y-\\hat{\\beta_1}S_XS_X-S_XS_E-nS_{YX}+nS_{XE}$$\n",
    "\n",
    "$$\\hat{\\beta_1}(S_XS_X-nS_{XX}) = S_XS_Y-S_XS_E-nS_{YX}+nS_{XE}$$\n",
    "\n",
    "$$\\hat{\\beta_1}=\\frac{-nS_{YX}+S_XS_Y+nS_{XE}-S_XS_E}{S_XS_X-nS_{XX}}$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "$$RSS=\\sum(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i -\\varepsilon_i)^2=\\sum{( y_i^2-\\hat{\\beta_0}y_i-\\hat{\\beta_1}y_ix_i-y_i\\varepsilon_i-\\hat{\\beta_0}y_i+\\hat{\\beta_0}^2+\\hat{\\beta_0}\\hat{\\beta_1}x_i+\\hat{\\beta_0}\\varepsilon_i-\\hat{\\beta_1}x_iy_i+\\hat{\\beta_0}\\hat{\\beta_1}x_i+\\hat{\\beta_1}^2x_i^2+\\hat{\\beta_1}x_i\\varepsilon_i-\\varepsilon_iy_i+\\hat{\\beta_0}\\varepsilon_i+\\hat{\\beta_1}x_i\\varepsilon_i+\\varepsilon_i^2 )}=\\sum{( y_i^2-2\\hat{\\beta_0}y_i-2\\hat{\\beta_1}y_ix_i+2\\hat{\\beta_0}\\hat{\\beta_1}x_i+\\hat{\\beta_1}^2x_i^2+\\hat{\\beta_0}^2-2\\varepsilon_iy_i+2\\hat{\\beta_0}\\varepsilon_i+2\\hat{\\beta_1}x_i\\varepsilon_i+\\varepsilon^2_i )}$$\n",
    "\n",
    "Взяв частные производные по весам, получим:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_0}}RSS=-2\\sum{y_i}+2\\hat{\\beta_1}\\sum{x_i}+2\\sum{\\hat{\\beta_0}}+2\\sum{\\varepsilon_i}$$\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta_1}}RSS=2\\hat{\\beta_1}\\sum{x^2_i}+2\\hat{\\beta_0}\\sum{x_i}-2\\sum{y_ix_i}+2\\sum{x_i\\varepsilon_i}$$\n",
    "\n",
    "Для упрощения нотации запишем:\n",
    "\n",
    "$$S_X=\\sum{x_i}, S_Y=\\sum{y_i}, S_{YX}=\\sum{y_ix_i}, S_{XX}=\\sum{x^2_i}, nb_0=\\sum{\\hat{\\beta_0}}, S_E=\\sum{\\varepsilon_i}, S_{XE}=\\sum{x_i\\varepsilon_i}$$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\\hat{\\beta_0}=b_0=\\frac{S_Y-\\hat{\\beta_1}S_X - S_E}{n} \\implies S_Y=\\hat{\\beta_1}S_X+nb_0+S_E \\quad; \\quad \\hat{\\beta_1}=\\frac{S_{YX} - \\hat{\\beta_0}S_X-S_{XE}}{S_{XX}} \\implies S_{YX} = \\hat{\\beta_1}S_{XX} + \\hat{\\beta_0}S_X+S_{XE}$$\n",
    "\n",
    "Подставив выражение для $\\hat{\\beta_0}$ в выражение для  $\\hat{\\beta_1}$, получим:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Наблюдение первое - влияние шума на оценки\n",
    "\n",
    "Сначала рассмотрим влияние стохастического шума на оценки коэффициентов регрессии $\\hat{\\beta}$ на примере наклона -  $\\hat{\\beta_1}$. Выводя формулу, получим следующую ситуацию:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Отсюда\n",
    "\n",
    "$$\\hat{\\beta_0}=b_0=\\frac{S_Y-\\hat{\\beta_1}S_X}{n} \\implies S_Y=\\hat{\\beta_1}S_X+nb_0 \\quad\\quad; \\quad\\quad \\hat{\\beta_1}=\\frac{S_{YX} - \\hat{\\beta_0}S_X}{S_{XX}} \\implies S_{YX} = \\hat{\\beta_1}S_{XX} + \\hat{\\beta_0}S_X $$\n",
    "\n",
    "Подставив выражение для $\\hat{\\beta_0}$ в выражение для  $\\hat{\\beta_1}$, получим:\n",
    "\n",
    "$$0=\\hat{\\beta_1}S_{XX}+S_X\\frac{S_Y-\\hat{\\beta_1}S_X}{n}-S_{YX}$$\n",
    "$$0=\\hat{\\beta_1}S_{XX}+\\frac{S_XS_Y}{n}-\\frac{\\hat{\\beta_1}S_XS_X}{n}-S_{YX}$$\n",
    "$$0=n\\hat{\\beta_1}S_{XX}+S_XS_Y-\\hat{\\beta_1}S_XS_X-nS_{YX}$$\n",
    "$$\\hat{\\beta_1}(nS_{XX}-S_XS_X) = nS_{YX}-S_XS_Y$$\n",
    "<br>\n",
    "$$\\hat{\\beta_1} = \\frac{nS_{YX}-S_XS_Y}{nS_{XX}-S_XS_X} = \\frac{n\\sum{y_ix_i} - \\sum{x_i}\\sum{y_i}}{n\\sum{x^2_i}-(\\sum{x_i})^2}$$\n",
    "\n",
    "Рассмотрим числитель и знаменатель отдельно. Начнём со знаменателя, воспользовавшись формулой \"Var[Х] = mean of square minus square of mean\":\n",
    "\n",
    "$$n\\sum{x^2_i}-(\\sum{x_i})^2=n\\cdot{n}\\cdot \\underbrace{ \\frac{1}{n}\\sum{x^2_i} }_{\\mathrm{E}[X^2]} - n\\cdot \\underbrace{ \\frac{1}{n}\\sum{x_i} }_{\\mathrm{E}[X]} \\cdot n\\cdot \\underbrace{ \\frac{1}{n}\\sum{x_i} }_{\\mathrm{E}[X]} = n^2(\\mathrm{E}[X^2] - (\\mathrm{E}[X])^2) = n^2 \\cdot \\mathrm{Var}(X)$$\n",
    "\n",
    "Что можно расписать как\n",
    "\n",
    "$$n^2 \\cdot \\mathrm{Var}(X)=n^2 \\cdot \\frac{1}{n}\\sum(x_i-\\mathrm{E}[X])^2 = n \\cdot \\sum{(x_i-\\bar{x})^2}$$\n",
    "\n",
    "Теперь рассмотрим числитель. Там имеем:\n",
    "$$n\\sum{y_ix_i} = n \\cdot n\\cdot \\underbrace{ \\frac{1}{n}\\sum{x_iy_i} }_{\\mathrm{E}[XY]} = n^2\\cdot\\mathrm{E}[XY]$$\n",
    "$$\\sum{x_i}\\sum{y_i} = n\\cdot\\underbrace{ \\frac{1}{n}\\sum{x_i} }_{\\mathrm{E}[X]} \\cdot n\\cdot \\underbrace{ \\frac{1}{n}\\sum{y_i} }_{\\mathrm{E}[Y]}=n^2\\cdot\\mathrm{E}[X]\\mathrm{E}[Y]$$\n",
    "\n",
    "Итого в числителе получим:\n",
    "\n",
    "$$n\\sum{y_ix_i} - \\sum{x_i}\\sum{y_i}=n^2\\cdot\\mathrm{E}[XY] - n^2\\cdot\\mathrm{E}[X]\\mathrm{E}[Y] = n^2\\cdot(\\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y])$$\n",
    "\n",
    "Сразу бросается в глаза формула ковариации $Cov(X, Y) = \\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y] = \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y})$, воспользуемся ей:\n",
    "\n",
    "$$n^2\\cdot \\underbrace{ (\\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y]) }_{\\mathrm{Cov}(X, Y)} = n^2 \\cdot \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y}) = n\\cdot\\sum(x_i-\\bar{x})(y_i-\\bar{y}) $$\n",
    "\n",
    "Итого получим:\n",
    "<font color='maroon'>\n",
    "$$\\hat{\\beta_1} = \\frac{n\\cdot\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n \\cdot \\sum{(x_i-\\bar{x})^2}} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum{(x_i-\\bar{x})^2}}$$\n",
    "</font>\n",
    "\n",
    "Собственно, эту формулу мы и видим в книге ISLR.\n",
    "\n",
    "Нетрудно заметить из выкладок выше, что данную формулу можно также записать как:\n",
    "<br><br>\n",
    "<font color='maroon'>\n",
    "$$\\hat{\\beta_1} = \\frac{\\mathrm{Cov}(XY)}{\\mathrm{Var}[X]}$$\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строгая экзогенность\n",
    "<br>\n",
    "- **Вопрос 1:** если в целевой функции две зависимых переменных, а в модели построена только одна, то насколько конкретно будет смещена оценка коэффициента перед переменной в модели? Рассмотрим два случая: когда имеется confounding-переменная (т.е. истинная причина, которая влияет на зависимую и независимую переменные), и когда имеется просто недоступная нам переменная, которая не влияет на имеющуюся у нас переменную, однако при этом её матожидание не равно нулю, а оттуда и матожидание шума получится ненулевое, то есть предположение о строгой экзогенности будет нарушено.\n",
    "\n",
    "- **Вопрос 2:** если в целевой функции $\\text{y}$ зависит от $\\text{X и X^2}$, а в модели - только от $\\text{X}$ (то есть $\\text{X^2}$ окажется в шуме), то насколько конкретно будет смещена оценка коэффициента перед переменной в модели?\n",
    "\n",
    "<br><br><br><hr>\n",
    "## 1. Наблюдение первое - confounding variable и дополнительная переменная\n",
    "Рассмотрим следующий случай. Пусть на основе имеющихся данных мы регрессируем Зарплату на Образование, и построенная нами модель имеет вид:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot\\text{Education} + \\varepsilon$$\n",
    "\n",
    "Однако в реальности отношение несколько иное:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot \\text{Education} + \\beta_2\\cdot\\text{Ability} + \\mu$$\n",
    "\n",
    "Способности в данном случае влияют на Образование и на Зарплату: чем выше Способности - тем больше Образование (более способные ученики чаще получают лучшее образование) и тем больше прогнозируемая Зарплата (более способные люди чаще зарабатывают больше). Между тем, Образование само по себе не очень сильно влияет на Зарплату: можно учиться долго и много, но зарабатывать мало, тогда как несколько менее образованный но более \"пробивной\" работник может подняться выше по карьерной лестнице. Легко показать, что есть матожидание Способностей ненулевое, то $\\mathsf{E}[\\varepsilon]\\neq 0$, то есть нарушается экзогенность. Рассмотрим эту ситуацию подробнее и обратим внимание на оценки коэффициентов Образования.\n",
    "\n",
    "<br><br><center>**Опыт 1. Confounding-переменная, влияет на X и y**</center>\n",
    "\n",
    "Введём следующие условия:\n",
    "\n",
    "**Условия:**\n",
    "1. Зависимая переменная регрессируется на одну независимую переменную.\n",
    "2. Присутствует скрытая переменная, влияющая как на х, так и на у.\n",
    "\n",
    "**Объект интереса:**\n",
    "- Оценка коэффициента $\\text{x}$ и её смещение - endgogeneity bias \n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- ///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      " Сгенерирована целевая функция. Шум отсутствует.\n",
      "    Значения коэффициентов целевой функции:\n",
      "\t· β0 = 4.5 \n",
      "\t· β1 = 21.784015632708332\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Добавлен гауссовский шум.\n",
      "    Матожидание шума E[epsilon]:\n",
      "\t\tРеальное     : 10 \n",
      "\t\tРассчитанное : 13.4449794699 (на 1000 наблюдений)\n",
      "    Энергия шума   Var[epsilon]:\n",
      "\t\tРеальная     : 10000 \n",
      "\t\tРассчитанная : 9657.73442387 (на 1000 наблюдений)\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Произведена подгонка модели.\n",
      "\t· Оценка β0: 17.8761867374\n",
      "\t· Оценка β1: 20.4308836191\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      " Выполнены рассчёты.\n",
      "\tОценка β1 на чистых данных : 21.784015632708332\n",
      "\tОценка β1 на шумных данных : 20.4308836191\n",
      "\tРеальная разность β        : -1.35313201358\n",
      "\tВычисленная разность β     : -1.35313201358\n",
      "\n",
      "\tТаким образом, согласно разложению:\n",
      "\n",
      "\t\t\t                   Cov(X, E)   \n",
      "\t\t\tОценка_β1 = β1 + ——————————————\n",
      "\t\t\t                   (std(X))^2  \n",
      "\n",
      "\tИмеем:\n",
      "\t\t\tОценка_β1 = 20.4308836191\n",
      "\n",
      "\n",
      "Рассчитанное матожидание резидуалов: -7.04858393874e-15\n",
      "==== dop: ==== \n",
      "20.4308836191\n",
      "-----\n",
      "20.4308836191\n",
      "\n",
      "----\n",
      "У меня:\n",
      "3392.5086801 16837.48815\n",
      "21753.4739365\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import numpy as np\n",
    "# Graphics:\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "# ML:\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import make_regression\n",
    "# Wargnings:\n",
    "np.warnings.filterwarnings(\"ignore\")\n",
    "from pandas.core import datetools\n",
    "np.warnings.resetwarnings()\n",
    "\n",
    "PRINT_LINE = '-'*80 + '\\n'\n",
    "\n",
    "POPULATION_SIZE = 1000\n",
    "BIAS       = 4.5\n",
    "NOISE_TYPE = 'gaussian' # or 'uniform'\n",
    "LOW, HI    = -10, 55\n",
    "NOISE_MEAN, NOISE_STD = 10, 100\n",
    "\n",
    "# 1. Сгенерируем данные без шума:\n",
    "X, y, real_coef = make_regression(n_samples=POPULATION_SIZE, n_features=1, n_informative=1, \n",
    "                                  coef=True, noise=0, bias=BIAS)\n",
    "print(PRINT_LINE, 'Сгенерирована целевая функция. Шум отсутствует.\\n    Значения коэффициентов целевой функции:')\n",
    "print('\\t· β0 =', BIAS, '\\n\\t· β1 =', real_coef.item(0))\n",
    "\n",
    "# 2. Добавим шум вручную (это позволит нам сохранить его для исследования)\n",
    "if NOISE_TYPE == 'gaussian':\n",
    "    epsilon, typestring = np.random.normal(NOISE_MEAN, NOISE_STD, y.shape), 'гауссовский'\n",
    "    mu, std = NOISE_MEAN, NOISE_STD\n",
    "elif NOISE_TYPE == 'uniform':\n",
    "    epsilon, typestring = np.random.uniform(LOW, HI, y.shape), 'равномерный'\n",
    "    mu, std = 0.5(LOW + HI), np.sqrt(((HI-LOW)**2)/12)\n",
    "\n",
    "y_noisy = y + epsilon\n",
    "print('\\n', PRINT_LINE, 'Добавлен', typestring, 'шум.')\n",
    "print('    Матожидание шума E[epsilon]:')\n",
    "print('\\t\\tРеальное     :', mu, '\\n\\t\\tРассчитанное :', np.mean(epsilon), \n",
    "      '(на', len(epsilon),'наблюдений)')\n",
    "print('    Энергия шума   Var[epsilon]:')\n",
    "print('\\t\\tРеальная     :', std**2, '\\n\\t\\tРассчитанная :', np.var(epsilon, ddof=1), \n",
    "      '(на', len(epsilon),'наблюдений)',)\n",
    "\n",
    "# 3. Произведём подгонку модели к зашумлённой версии y:\n",
    "statsmodels_result = sm.OLS(y_noisy, sm.add_constant(X)).fit()\n",
    "print('\\n', PRINT_LINE, 'Произведена подгонка модели.\\n\\t· Оценка β0:', statsmodels_result.params[0])\n",
    "print('\\t· Оценка β1:', statsmodels_result.params[1])\n",
    "\n",
    "# 4. Выполним рассчёты:\n",
    "print('\\n', PRINT_LINE, 'Выполнены рассчёты.')\n",
    "print('\\tОценка β1 на чистых данных :', real_coef.item(0))\n",
    "# print('\\t\\tCov(X,Y)/Var[X] =', np.cov(X[:, 0], y)[0, 1]/np.var(X, ddof=1))\n",
    "print('\\tОценка β1 на шумных данных :', statsmodels_result.params[1])\n",
    "# print('\\t\\tCov(X,Y)/Var[X] =', np.cov(X[:, 0], y_noisy)[0, 1]/np.var(X, ddof=1))\n",
    "print('\\tРеальная разность β        :', statsmodels_result.params[1]-real_coef.item(0))\n",
    "cov = np.cov(X[:, 0], epsilon)[0, 1]\n",
    "var = np.var(X, ddof=1)\n",
    "print('\\tВычисленная разность β     :', cov/var)\n",
    "print('\\n\\tТаким образом, согласно разложению:\\n')\n",
    "print('\\t\\t\\t                   Cov(X, E)   ')\n",
    "print('\\t\\t\\tОценка_β1 = β1 + ——————————————')\n",
    "print('\\t\\t\\t                   (std(X))^2  ')\n",
    "print('\\n\\tИмеем:')\n",
    "print('\\t\\t\\tОценка_β1 =', real_coef.item(0) + cov/var)\n",
    "print('\\n\\nРассчитанное матожидание резидуалов:', np.mean(statsmodels_result.resid))\n",
    "\n",
    "\n",
    "print('==== dop: ==== ')\n",
    "n = len(X)\n",
    "x = X[:, 0]\n",
    "numer = n*np.dot(x, y) - sum(x)*sum(y) - sum(x)*sum(epsilon) + n*np.dot(x, epsilon)\n",
    "denom = n*np.dot(x, x) - sum(x)*sum(x)\n",
    "print(numer/denom)\n",
    "print('-----')\n",
    "numer = np.cov(x, y)[0, 1] + np.cov(x, epsilon)[0, 1]\n",
    "denom = np.var(x, ddof=1)\n",
    "print(numer/denom)\n",
    "\n",
    "\n",
    "print('\\n----\\nУ меня:')\n",
    "\n",
    "b0_hat, b1_hat = statsmodels_result.params[0], statsmodels_result.params[1]\n",
    "#b0_hat, b1_hat = BIAS, real_coef.item(0)\n",
    "print(sum(y), sum(y_noisy))\n",
    "print(b0_hat*sum(x) + b1_hat*np.dot(x, x) + )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "informative_beta     = max(real_coef.item(0), real_coef.item(1))\n",
    "non_informative_beta = min(real_coef.item(0), real_coef.item(1))\n",
    "\n",
    "x = X[:, informative_feature_index]\n",
    "z = X[:, 1-informative_feature_index]\n",
    "\n",
    "# Длинная запись:\n",
    "# xz_dot = np.dot(x, z)                  # 1st term\n",
    "# z_mean_x = z*np.mean(x)\n",
    "# z_mean_x = sum(z_mean_x)               # 2nd term\n",
    "# norm_x = (np.linalg.norm(x))**2        # 3rd term\n",
    "# n_mean_x_sq = len(x) * (np.mean(x))**2 # 4th term\n",
    "# b1_biased_estimate = non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq))\n",
    "# print(':::', non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq)))\n",
    "\n",
    "# Короткая запись:\n",
    "numerator   = np.dot(np.array(x)-np.mean(x), z)\n",
    "denominator = np.square(np.linalg.norm(x)) - len(x)*np.square(np.mean(x)) \n",
    "b1_biased_estimate = non_informative_beta + informative_beta*(numerator/denominator)\n",
    "\n",
    "print('Biased b1 estimate:', b1_biased_estimate)\n",
    "print('Absolute difference:', np.abs(b1_estimate - b1_biased_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([1, 2]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimated coefficient b would pick up the effect of x on y plus the association of x and z times the effect of z on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z_X = np.delete(X, informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('Effect of NON-INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "# z_X = np.delete(X, 1-informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('\\nEffect of INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "\n",
    "# z_X_1, z_X_2 = X[informative_feature_index], X[1 - informative_feature_index]\n",
    "# statsmodels_result = sm.OLS(z_X_2, sm.add_constant(z_X_1)).fit()\n",
    "# print('\\nEffect of NON-INFORMATIVE on INFORMATIVE:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(z_X_2, z_X_1).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noninformative_coef + (inter_coef * informative_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-1.76683487328 + (-0.073345030119 * 20.9079770399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a)\n",
    "a[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат:**\n",
    "* MSE на тренировочной выборке ожидаемо получилась практически нулевой, в силу того, что подгонка полинома второй степени к двум точкам всегда будет идеальной.\n",
    "* Однако на популяции MSE ожидаемо оказалась выше, с мощной дисперсией. Это обусловлено тем, что через две точки можно провести бесконечное количество парабол, и какая из них будет ближе к целевой функции - абсолютно неизвестно.\n",
    "\n",
    "Теперь проведём такой же опыт, но уберём вариативность, задав как минимум 3 точки для тренировочного набора. Для точного задания параболы необходимо иметь как минимум 3 точки, через которые она проходит; а в силу того, что данные не имеют шума, эти 3 точки всегда будут лежать ровно на целевой параболе. Таким образом, ожидается, что подгонка всегда будет идеальной.\n",
    "\n",
    "<br><br><center>**Опыт 2. Нулевой шум, нулевая вариативность**</center>\n",
    "\n",
    "**Условия:**\n",
    "1. Стохастический шум равен нулю: $\\forall{x}, \\epsilon(x)=0$\n",
    "2. Детерминистский шум равен нулю: $\\mathrm{Complexity}(g)=\\mathrm{Complexity}(f)$\n",
    "3. Вариативность модели нулевая; в данном случае (т.е. в случае с целевой функцией-параболой) объём выборки должен быть $\\geq 3$.\n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- Oжидается нулевая MSE как на тренировке, так и на популяции. Это будет достигнуто за счёт нулевой вариативности, нулевого детерминистского и нулевого стохастического шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
