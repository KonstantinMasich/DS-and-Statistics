{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строгая экзогенность\n",
    "<br>\n",
    "- **Вопрос 1:** если в целевой функции две зависимых переменных, а в модели построена только одна, то насколько конкретно будет смещена оценка коэффициента перед переменной в модели? Рассмотрим два случая: когда имеется confounding-переменная (т.е. истинная причина, которая влияет на зависимую и независимую переменные), и когда имеется просто недоступная нам переменная, которая не влияет на имеющуюся у нас переменную, однако при этом её матожидание не равно нулю, а оттуда и матожидание шума получится ненулевое, то есть предположение о строгой экзогенности будет нарушено.\n",
    "\n",
    "- **Вопрос 2:** если в целевой функции $\\text{y}$ зависит от $\\text{X и X^2}$, а в модели - только от $\\text{X}$ (то есть $\\text{X^2}$ окажется в шуме), то насколько конкретно будет смещена оценка коэффициента перед переменной в модели?\n",
    "\n",
    "<br><br><br><hr>\n",
    "## 1. Наблюдение первое - confounding variable и дополнительная переменная\n",
    "Рассмотрим следующий случай. Пусть на основе имеющихся данных мы регрессируем Зарплату на Образование, и построенная нами модель имеет вид:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot\\text{Education} + \\varepsilon$$\n",
    "\n",
    "Однако в реальности отношение несколько иное:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot \\text{Education} + \\beta_2\\cdot\\text{Ability} + \\mu$$\n",
    "\n",
    "Способности в данном случае влияют на Образование и на Зарплату: чем выше Способности - тем больше Образование (более способные ученики чаще получают лучшее образование) и тем больше прогнозируемая Зарплата (более способные люди чаще зарабатывают больше). Между тем, Образование само по себе не очень сильно влияет на Зарплату: можно учиться долго и много, но зарабатывать мало, тогда как несколько менее образованный но более \"пробивной\" работник может подняться выше по карьерной лестнице. Легко показать, что есть матожидание Способностей ненулевое, то $\\mathsf{E}[\\varepsilon]\\neq 0$, то есть нарушается экзогенность. Рассмотрим эту ситуацию подробнее и обратим внимание на оценки коэффициентов Образования.\n",
    "\n",
    "<br><br><center>**Опыт 1. Confounding-переменная, влияет на X и y**</center>\n",
    "\n",
    "Введём следующие условия:\n",
    "\n",
    "**Условия:**\n",
    "1. Зависимая переменная регрессируется на одну независимую переменную.\n",
    "2. Присутствует скрытая переменная, влияющая как на х, так и на у.\n",
    "\n",
    "**Объект интереса:**\n",
    "- Оценка коэффициента $\\text{x}$ и её смещение - endgogeneity bias \n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- ///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  -- Сгенерирована целевая функция.\n",
      " \u001b[0m     Значения коэффициентов целевой функции:\n",
      "\t· β0 = 4.5 \n",
      "\t· β1 = 0.0 \n",
      "\t· β2 = 21.553976954923993\n",
      "    Корреляция между информативным и неинформативным признаками: -0.0373502502487\n",
      "    Корреляция между информативным признаком и y:                0.862109803766\n",
      "    Корреляция между неинформативным признаком и y:              -0.073345030119\n",
      "\u001b[1m \n",
      "\n",
      " -- Построена модель (на основании неинформативного признака). \u001b[0m\n",
      "    Оценки коэффициентов регрессии:\n",
      "\t· Оценка β0: 4.53800673976 \n",
      "\t· Оценка β1: -1.76683487328\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import numpy as np\n",
    "# Graphics:\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "# ML:\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import make_regression\n",
    "FONT_BOLD, FONT_END = '\\033[1m', '\\033[0m'\n",
    "\n",
    "BIAS  = 4.5\n",
    "NOISE = 12\n",
    "POPULATION_SIZE = 500\n",
    "SAMPLE_SIZE     = int(POPULATION_SIZE/5)\n",
    "\n",
    "\n",
    "# 1. Сгенерируем проблему регрессии с двумя независимыми переменными:\n",
    "X, y, real_coef = make_regression(n_samples=POPULATION_SIZE, n_features=2, n_informative=1, \n",
    "                                  coef=True, noise=NOISE, bias=BIAS)\n",
    "print(FONT_BOLD, ' -- Сгенерирована целевая функция.\\n', FONT_END, '    Значения коэффициентов целевой функции:')\n",
    "print('\\t· β0 =', BIAS, '\\n\\t· β1 =', real_coef.item(0), '\\n\\t· β2 =', real_coef.item(1))\n",
    "informative_feature_index = int(real_coef.item(0) == 0)\n",
    "informative_feature_coef  = max(real_coef.item(0), real_coef.item(1))\n",
    "\n",
    "# 2. Запишем коэффициент корреляции Пирсона между информативным и неинформативным признаками:\n",
    "inter_coef          = np.corrcoef([row[0] for row in X], [row[1] for row in X])[0, 1]\n",
    "informative_coef    = np.corrcoef([row[informative_feature_index] for row in X], y)[0, 1]\n",
    "noninformative_coef = np.corrcoef([row[1 - informative_feature_index] for row in X], y)[0, 1]\n",
    "print('    Корреляция между информативным и неинформативным признаками:', inter_coef)\n",
    "print('    Корреляция между информативным признаком и y:', ' '*14, informative_coef)\n",
    "print('    Корреляция между неинформативным признаком и y:', ' '*12, noninformative_coef)\n",
    "\n",
    "# 3. Экстрагируем информативный признак, и регрессируем y на неинформативный признак:\n",
    "new_X = np.delete(X, informative_feature_index, 1)\n",
    "statsmodels_result = sm.OLS(y, sm.add_constant(new_X)).fit()\n",
    "print(FONT_BOLD, '\\n\\n -- Построена модель (на основании неинформативного признака).', FONT_END)\n",
    "print('    Оценки коэффициентов регрессии:\\n\\t· Оценка β0:', statsmodels_result.params[0], \n",
    "      '\\n\\t· Оценка β1:', statsmodels_result.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of NON-INFORMATIVE on y:\n",
      "\tWith bias: -1.76683487328\n",
      "\tNo bias:   -1.44801061253\n",
      "\n",
      "Effect of INFORMATIVE on y:\n",
      "\tWith bias: 20.9079770399\n",
      "\tNo bias:   20.9251408303\n",
      "\n",
      "Effect of INFORMATIVE on NON-INFORMATIVE:\n",
      "\tWith bias: -2.35648757822\n",
      "\tNo bias:   -2.5007427365\n"
     ]
    }
   ],
   "source": [
    "z_X = np.delete(X, informative_feature_index, 1)\n",
    "statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "print('Effect of NON-INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "z_X = np.delete(X, 1-informative_feature_index, 1)\n",
    "statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "print('\\nEffect of INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "\n",
    "z_X_1, z_X_2 = X[informative_feature_index], X[1 - informative_feature_index]\n",
    "statsmodels_result = sm.OLS(z_X_2, sm.add_constant(z_X_1)).fit()\n",
    "print('\\nEffect of NON-INFORMATIVE on INFORMATIVE:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "statsmodels_result = sm.OLS(z_X_2, z_X_1).fit()\n",
    "print('\\tNo bias:  ', statsmodels_result.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimated coefficient b would pick up the effect of x on y plus the association of x and z times the effect of z on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10554504703150308"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noninformative_coef + (inter_coef * informative_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.300331078998826"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.76683487328 + (-0.073345030119 * 20.9079770399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a)\n",
    "a[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат:**\n",
    "* MSE на тренировочной выборке ожидаемо получилась практически нулевой, в силу того, что подгонка полинома второй степени к двум точкам всегда будет идеальной.\n",
    "* Однако на популяции MSE ожидаемо оказалась выше, с мощной дисперсией. Это обусловлено тем, что через две точки можно провести бесконечное количество парабол, и какая из них будет ближе к целевой функции - абсолютно неизвестно.\n",
    "\n",
    "Теперь проведём такой же опыт, но уберём вариативность, задав как минимум 3 точки для тренировочного набора. Для точного задания параболы необходимо иметь как минимум 3 точки, через которые она проходит; а в силу того, что данные не имеют шума, эти 3 точки всегда будут лежать ровно на целевой параболе. Таким образом, ожидается, что подгонка всегда будет идеальной.\n",
    "\n",
    "<br><br><center>**Опыт 2. Нулевой шум, нулевая вариативность**</center>\n",
    "\n",
    "**Условия:**\n",
    "1. Стохастический шум равен нулю: $\\forall{x}, \\epsilon(x)=0$\n",
    "2. Детерминистский шум равен нулю: $\\mathrm{Complexity}(g)=\\mathrm{Complexity}(f)$\n",
    "3. Вариативность модели нулевая; в данном случае (т.е. в случае с целевой функцией-параболой) объём выборки должен быть $\\geq 3$.\n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- Oжидается нулевая MSE как на тренировке, так и на популяции. Это будет достигнуто за счёт нулевой вариативности, нулевого детерминистского и нулевого стохастического шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
