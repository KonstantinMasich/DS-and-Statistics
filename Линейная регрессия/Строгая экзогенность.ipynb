{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строгая экзогенность\n",
    "<br>\n",
    "- **Вопрос 1:** если в целевой функции две зависимых переменных, а в модели построена только одна, то насколько конкретно будет смещена оценка коэффициента перед переменной в модели? Рассмотрим два случая: когда имеется confounding-переменная (т.е. истинная причина, которая влияет на зависимую и независимую переменные), и когда имеется просто недоступная нам переменная, которая не влияет на имеющуюся у нас переменную, однако при этом её матожидание не равно нулю, а оттуда и матожидание шума получится ненулевое, то есть предположение о строгой экзогенности будет нарушено.\n",
    "\n",
    "- **Вопрос 2:** если в целевой функции $\\text{y}$ зависит от $\\text{X и X^2}$, а в модели - только от $\\text{X}$ (то есть $\\text{X^2}$ окажется в шуме), то насколько конкретно будет смещена оценка коэффициента перед переменной в модели?\n",
    "\n",
    "<br><br><br><hr>\n",
    "## 1. Наблюдение первое - confounding variable и дополнительная переменная\n",
    "Рассмотрим следующий случай. Пусть на основе имеющихся данных мы регрессируем Зарплату на Образование, и построенная нами модель имеет вид:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot\\text{Education} + \\varepsilon$$\n",
    "\n",
    "Однако в реальности отношение несколько иное:\n",
    "\n",
    "$$\\text{Wages} = \\beta_0 + \\beta_1\\cdot \\text{Education} + \\beta_2\\cdot\\text{Ability} + \\mu$$\n",
    "\n",
    "Способности в данном случае влияют на Образование и на Зарплату: чем выше Способности - тем больше Образование (более способные ученики чаще получают лучшее образование) и тем больше прогнозируемая Зарплата (более способные люди чаще зарабатывают больше). Между тем, Образование само по себе не очень сильно влияет на Зарплату: можно учиться долго и много, но зарабатывать мало, тогда как несколько менее образованный но более \"пробивной\" работник может подняться выше по карьерной лестнице. Легко показать, что есть матожидание Способностей ненулевое, то $\\mathsf{E}[\\varepsilon]\\neq 0$, то есть нарушается экзогенность. Рассмотрим эту ситуацию подробнее и обратим внимание на оценки коэффициентов Образования.\n",
    "\n",
    "<br><br><center>**Опыт 1. Confounding-переменная, влияет на X и y**</center>\n",
    "\n",
    "Введём следующие условия:\n",
    "\n",
    "**Условия:**\n",
    "1. Зависимая переменная регрессируется на одну независимую переменную.\n",
    "2. Присутствует скрытая переменная, влияющая как на х, так и на у.\n",
    "\n",
    "**Объект интереса:**\n",
    "- Оценка коэффициента $\\text{x}$ и её смещение - endgogeneity bias \n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- ///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 10000\n",
      "\u001b[1m  -- Сгенерирована целевая функция.\n",
      " \u001b[0m     Значения коэффициентов целевой функции:\n",
      "\t· β0 = 4.5 \n",
      "\t· β1 = 27.413748556543705 \n",
      "\t· β2 = 0.0\n",
      "    Корреляция между информативным и неинформативным признаками: -0.013473743768540714\n",
      "    Корреляция между информативным признаком и y:                0.9173594166375743\n",
      "    Корреляция между неинформативным признаком и y:              -0.019537410915966787\n",
      "\u001b[1m \n",
      "\n",
      " -- Построена модель (на основании неинформативного признака). \u001b[0m\n",
      "    Оценки коэффициентов регрессии:\n",
      "\t· Оценка β0: 4.633008952552597 \n",
      "\t· Оценка β1: -0.5895114532297282\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import numpy as np\n",
    "# Graphics:\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "# ML:\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import make_regression\n",
    "FONT_BOLD, FONT_END = '\\033[1m', '\\033[0m'\n",
    "\n",
    "BIAS  = 4.5\n",
    "NOISE = 12\n",
    "POPULATION_SIZE = 10000\n",
    "print('Used', POPULATION_SIZE)\n",
    "\n",
    "\n",
    "# 1. Сгенерируем проблему регрессии с двумя независимыми переменными:\n",
    "X, y, real_coef = make_regression(n_samples=POPULATION_SIZE, n_features=2, n_informative=1, \n",
    "                                  coef=True, noise=NOISE, bias=BIAS)\n",
    "print(FONT_BOLD, ' -- Сгенерирована целевая функция.\\n', FONT_END, '    Значения коэффициентов целевой функции:')\n",
    "print('\\t· β0 =', BIAS, '\\n\\t· β1 =', real_coef.item(0), '\\n\\t· β2 =', real_coef.item(1))\n",
    "informative_feature_index = int(real_coef.item(0) == 0)\n",
    "informative_feature_coef  = max(real_coef.item(0), real_coef.item(1))\n",
    "\n",
    "# 2. Запишем коэффициент корреляции Пирсона между информативным и неинформативным признаками:\n",
    "inter_coef          = np.corrcoef([row[0] for row in X], [row[1] for row in X])[0, 1]\n",
    "informative_coef    = np.corrcoef([row[informative_feature_index] for row in X], y)[0, 1]\n",
    "noninformative_coef = np.corrcoef([row[1 - informative_feature_index] for row in X], y)[0, 1]\n",
    "print('    Корреляция между информативным и неинформативным признаками:', inter_coef)\n",
    "print('    Корреляция между информативным признаком и y:', ' '*14, informative_coef)\n",
    "print('    Корреляция между неинформативным признаком и y:', ' '*12, noninformative_coef)\n",
    "\n",
    "# 3. Экстрагируем информативный признак, и регрессируем y на неинформативный признак:\n",
    "new_X = np.delete(X, informative_feature_index, 1)\n",
    "statsmodels_result = sm.OLS(y, sm.add_constant(new_X)).fit()\n",
    "print(FONT_BOLD, '\\n\\n -- Построена модель (на основании неинформативного признака).', FONT_END)\n",
    "print('    Оценки коэффициентов регрессии:\\n\\t· Оценка β0:', statsmodels_result.params[0], \n",
    "      '\\n\\t· Оценка β1:', statsmodels_result.params[1])\n",
    "\n",
    "b1_estimate = statsmodels_result.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased b1 estimate: -0.3667157829263108\n",
      "Absolute difference: 0.2227956703034174\n"
     ]
    }
   ],
   "source": [
    "informative_beta     = max(real_coef.item(0), real_coef.item(1))\n",
    "non_informative_beta = min(real_coef.item(0), real_coef.item(1))\n",
    "\n",
    "x = X[:, informative_feature_index]\n",
    "z = X[:, 1-informative_feature_index]\n",
    "\n",
    "# Длинная запись:\n",
    "# xz_dot = np.dot(x, z)                  # 1st term\n",
    "# z_mean_x = z*np.mean(x)\n",
    "# z_mean_x = sum(z_mean_x)               # 2nd term\n",
    "# norm_x = (np.linalg.norm(x))**2        # 3rd term\n",
    "# n_mean_x_sq = len(x) * (np.mean(x))**2 # 4th term\n",
    "# b1_biased_estimate = non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq))\n",
    "# print(':::', non_informative_beta + informative_beta*((xz_dot-z_mean_x) / (norm_x+n_mean_x_sq)))\n",
    "\n",
    "# Короткая запись:\n",
    "numerator   = np.dot(np.array(x)-np.mean(x), z)\n",
    "denominator = np.square(np.linalg.norm(x)) - len(x)*np.square(np.mean(x)) \n",
    "b1_biased_estimate = non_informative_beta + informative_beta*(numerator/denominator)\n",
    "\n",
    "print('Biased b1 estimate:', b1_biased_estimate)\n",
    "print('Absolute difference:', np.abs(b1_estimate - b1_biased_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimated coefficient b would pick up the effect of x on y plus the association of x and z times the effect of z on y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_X = np.delete(X, informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('Effect of NON-INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "# z_X = np.delete(X, 1-informative_feature_index, 1)\n",
    "# statsmodels_result = sm.OLS(y, sm.add_constant(z_X)).fit()\n",
    "# print('\\nEffect of INFORMATIVE on y:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(y, z_X).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])\n",
    "\n",
    "\n",
    "# z_X_1, z_X_2 = X[informative_feature_index], X[1 - informative_feature_index]\n",
    "# statsmodels_result = sm.OLS(z_X_2, sm.add_constant(z_X_1)).fit()\n",
    "# print('\\nEffect of NON-INFORMATIVE on INFORMATIVE:\\n\\tWith bias:', statsmodels_result.params[1])\n",
    "# statsmodels_result = sm.OLS(z_X_2, z_X_1).fit()\n",
    "# print('\\tNo bias:  ', statsmodels_result.params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.70170091, -1.85892225],\n",
       "       [ 0.08731626, -0.73426896]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10554504703150308"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noninformative_coef + (inter_coef * informative_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.300331078998826"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.76683487328 + (-0.073345030119 * 20.9079770399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a)\n",
    "a[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат:**\n",
    "* MSE на тренировочной выборке ожидаемо получилась практически нулевой, в силу того, что подгонка полинома второй степени к двум точкам всегда будет идеальной.\n",
    "* Однако на популяции MSE ожидаемо оказалась выше, с мощной дисперсией. Это обусловлено тем, что через две точки можно провести бесконечное количество парабол, и какая из них будет ближе к целевой функции - абсолютно неизвестно.\n",
    "\n",
    "Теперь проведём такой же опыт, но уберём вариативность, задав как минимум 3 точки для тренировочного набора. Для точного задания параболы необходимо иметь как минимум 3 точки, через которые она проходит; а в силу того, что данные не имеют шума, эти 3 точки всегда будут лежать ровно на целевой параболе. Таким образом, ожидается, что подгонка всегда будет идеальной.\n",
    "\n",
    "<br><br><center>**Опыт 2. Нулевой шум, нулевая вариативность**</center>\n",
    "\n",
    "**Условия:**\n",
    "1. Стохастический шум равен нулю: $\\forall{x}, \\epsilon(x)=0$\n",
    "2. Детерминистский шум равен нулю: $\\mathrm{Complexity}(g)=\\mathrm{Complexity}(f)$\n",
    "3. Вариативность модели нулевая; в данном случае (т.е. в случае с целевой функцией-параболой) объём выборки должен быть $\\geq 3$.\n",
    "\n",
    "**Ожидание:**\n",
    "\n",
    "- Oжидается нулевая MSE как на тренировке, так и на популяции. Это будет достигнуто за счёт нулевой вариативности, нулевого детерминистского и нулевого стохастического шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
